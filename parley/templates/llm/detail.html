{% extends 'page.html' %}

{% block page-pretitle %}Models{% endblock %}
{% block page-title %}{{ llm.name }}{% endblock %}
{% block page-controls %}
  <a href="{% url 'admin:parley_llm_change' llm.id %}" class="btn btn-primary" title="Edit LLM Model">
    <span class="fe fe-edit"></span>
  </a>
{% endblock %}

{% block page %}
<div class="chart">
  <canvas id="metricsChart" class="chart-canvas"></canvas>
</div>

<!-- simple metrics table -->
<div class="row">
  <div class="col-12">
    <div class="card">
      <div class="card-header">
        <h4 class="card-header-title">Boolean Metrics</h4>
      </div>

      <div class="table-responsive mb-0">
        <table class="table table-sm table-nowrap card-table">
          <thead>
            <tr>
              <th>Evaluation</th>
              <th>Metric</th>
              <th class="text-center">Positive</th>
              <th class="text-center">Negative</th>
            </tr>
          </thead>
          <tbody>
            {% for eval in llm.model_evaluations.all %}
            {% if eval.metrics_cached %}
            {% if eval.valid_output_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Valid Output</td>
              <td class="text-center">{{ eval.n_valid_output_type }} ({{ eval.percent_valid_output_type }}%)</td>
              <td class="text-center">{{ eval.n_invalid_output_type }} ({{ eval.percent_invalid_output_type }}%)</td>
            </tr>
            {% endif %}
            {% if eval.sensitive_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Leaks Sensitive</td>
              <td class="text-center">{{ eval.n_leaks_sensitive }} ({{ eval.percent_leaks_sensitive }}%)</td>
              <td class="text-center">{{ eval.n_no_sensitive_leaks }} ({{ eval.percent_no_sensitive_leaks }}%)</td>
            </tr>
            {% endif %}
            {% if eval.similarity_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Similarity to Expected</td>
              <td class="text-center">{{ eval.n_similar }} ({{ eval.percent_similar }}%)</td>
              <td class="text-center">{{ eval.n_not_similar }} ({{ eval.percent_not_similar }}%)</td>
            </tr>
            {% endif %}
            {% if eval.labels_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Labeled Correctly</td>
              <td class="text-center">{{ eval.n_labeled_correctly }} ({{ eval.percent_labeled_correctly }}%)</td>
              <td class="text-center">{{ eval.n_labeled_incorrectly }} ({{ eval.percent_labeled_incorrectly }}%)</td>
            </tr>
            {% endif %}
            {% if eval.factual_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Is Factual</td>
              <td class="text-center">{{ eval.n_factual }} ({{ eval.percent_factual }}%)</td>
              <td class="text-center">{{ eval.n_not_factual }} ({{ eval.percent_not_factual }}%)</td>
            </tr>
            {% endif %}
            {% if eval.readability_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Readable Output</td>
              <td class="text-center">{{ eval.n_readable }} ({{ eval.percent_readable }}%)</td>
              <td class="text-center">{{ eval.n_not_readable }} ({{ eval.percent_not_readable }}%)</td>
            </tr>
            {% endif %}
            {% if eval.style_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Is Correct Style</td>
              <td class="text-center">{{ eval.n_correct_style }} ({{ eval.percent_correct_style }}%)</td>
              <td class="text-center">{{ eval.n_incorrect_style }} ({{ eval.percent_incorrect_style }}%)</td>
            </tr>
            {% endif %}
            {% endif %}
            {% empty %}
            <tr>
              <td colspan="4">No Evaluations Run</td>
            </tr>
            {% endfor %}
          </tbody>
        </table>
      </div>
    </div>
    <div class="card">
      <div class="card-header">
        <h4 class="card-header-title">Scalar Metrics</h4>
      </div>
      <div class="table-responsive mb-0">
        <table class="table table-sm table-nowrap card-table">
          <thead>
            <tr>
              <th>Evaluation</th>
              <th>Metric</th>
              <th class="text-center">Mean</th>
              <th class="text-center">Median</th>
              <th class="text-center">Total</th>
            </tr>
          </thead>
          <tbody>
            {% for eval in llm.model_evaluations.all %}
            {% if eval.metrics_cached %}
            {% if eval.helpfulness_processed %}
            <tr>
              <td>{{ eval.evaluation.name }}</td>
              <td>Helpfulness</td>
              <td class="text-center">{{ eval.mean_helpfulness }}</td>
              <td class="text-center">{{ eval.median_helpfulness }}</td>
              <td class="text-center">{{ eval.n_responses }}</td>
            </tr>
            {% endif %}
            {% endif %}
            {% endfor %}
          </tbody>
        </table>
      </div>
    </div>
  </div>
</div><!-- metrics table ends -->
{% endblock %}

{% block javascripts %}
  {{ block.super }}
  <script>
    new Chart('metricsChart', {
      type: 'bar',
      options: {
        barThickness: 24,
      },
      data: {
        labels: {{ chart.labels|safe }},
        datasets: {{ chart.datasets|safe }},
      }
    });
  </script>
{% endblock %}