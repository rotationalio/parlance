# parley.models.llm
# LLM models for evaluation and their responses.
#
# Author:   Benjamin Bengfort <benjamin@rotational.io>
# Created:  Sat Oct 05 15:11:26 2024 -0500
#
# Copyright (C) 2024 Rotational Labs, Inc.
# For license information, see LICENSE
#
# ID: llm.py [] benjamin@rotational.io $

"""
LLM models for evaluation and their responses.
"""

##########################################################################
## Imports
##########################################################################

import os
import json

from .base import BaseModel
from .enums import OutputFormat
from .enums import FivePointLikert

from django.db import models
from django.urls import reverse
from django.templatetags.static import static

from parley.models.user import ResponseReview
from parley.validators import validate_semver


##########################################################################
## Helpers
##########################################################################


def llm_cover_upload_path(instance, filename):
    _, ext = os.path.splitext(filename)
    return os.path.join("covers", "llms", f"{instance.id}{ext}")


##########################################################################
## Models
##########################################################################


class LLM(BaseModel):
    """
    A record of an instantiated, trained LLM model to evaluate.
    """

    name = models.CharField(
        default=None,
        null=False,
        blank=False,
        max_length=255,
        help_text="The name of the model or model family being evaluated",
    )

    version = models.CharField(
        default="0.1.0",
        null=False,
        blank=False,
        max_length=32,
        help_text="The semantic version of the model for instance identification",
        validators=[validate_semver],
    )

    description = models.TextField(
        null=True,
        blank=True,
        default=None,
        help_text="Any notes or other descriptive information about the model or training process",
    )

    cover_image = models.ImageField(
        null=True,
        blank=True,
        default=None,
        upload_to=llm_cover_upload_path,
        help_text="A 4x3 image representing the model for the profile page",
    )

    model_config = models.JSONField(
        null=True,
        default=None,
        blank=True,
        help_text="Configuration for instantiating the model",
    )

    generation_config = models.JSONField(
        null=True,
        default=None,
        blank=True,
        help_text="The standardized generation config of the model",
    )

    quantization_info = models.JSONField(
        null=True,
        default=None,
        blank=True,
        help_text="Information about the quantization of the model, if any",
    )

    tokenizer_config = models.JSONField(
        null=True,
        default=None,
        blank=True,
        help_text="The standardized tokenization info of the model",
    )

    max_new_tokens = models.IntegerField(
        null=True,
        default=None,
        editable=True,
        blank=True,
        help_text="The maximum new tokens allowed during inferencing",
    )

    is_adapter_model = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text="Defines if this model is a base model or a LoRA",
    )

    trained_on = models.DateTimeField(
        null=True,
        default=None,
        blank=True,
        help_text="The timestamp that the model started training",
    )

    training_duration = models.DurationField(
        null=True,
        default=None,
        blank=True,
        help_text="The amount of time it took to train the model",
    )

    evaluations = models.ManyToManyField(
        "parley.Evaluation",
        through="parley.ModelEvaluation",
    )

    class Meta:
        db_table = "llms"
        ordering = ("-trained_on",)
        get_latest_by = "trained_on"
        verbose_name = "LLM"
        verbose_name_plural = "LLMs"
        unique_together = ("name", "version")

    @property
    def training_completed(self):
        if self.trained_on is None or self.training_duration is None:
            return None
        return self.trained_on + self.training_duration

    def __str__(self):
        return self.name

    def get_absolute_url(self):
        return reverse("llm-detail", args=(self.id,))

    def get_cover_image(self):
        if self.cover_image:
            return self.cover_image.url
        return static("img/model-cover.png")


class Response(BaseModel):
    """
    A response is an output generated by an LLM to a specific prompt.
    """

    model = models.ForeignKey(
        "parley.LLM",
        null=False,
        on_delete=models.RESTRICT,
        related_name="responses",
        help_text="The LLM that generated the specified response",
    )

    prompt = models.ForeignKey(
        "parley.Prompt",
        null=False,
        on_delete=models.CASCADE,
        related_name="responses",
        help_text="The prompt that this is an LLM response to",
    )

    output = models.TextField(
        null=False,
        default=None,
        blank=False,
        help_text="The output generated by the LLM in response to the prompt",
    )

    output_similarity = models.FloatField(
        null=True,
        default=None,
        blank=True,
        help_text="The similarity score of this response to the expected output",
    )

    is_similar = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text="Was the similarity score greater than the threshold?",
    )

    label = models.CharField(
        null=True,
        blank=True,
        default=None,
        max_length=255,
        help_text="For classifiers, label that is extracted from the output",
    )

    label_correct = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text=(
            "Was the output label correct based on the expected label "
            "(or almost correct for fuzzy label matching)"
        ),
    )

    # For text - check to make sure it contains expected characters.
    valid_output_type = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text=(
            "Based on the expected output type, is it parseable; e.g. if the "
            "output is supposed to be JSON, can it be correctly decoded?",
        ),
    )

    leaks_sensitive = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        verbose_name="leaks sensitive data",
        help_text="Does the output contain sensitive data that should not be leaked?",
    )

    is_factual = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text="Is the output factually correct?",
    )

    is_readable = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text="Does the output contain grammatically correct, understandable language?",
    )

    is_correct_style = models.BooleanField(
        null=True,
        default=None,
        blank=True,
        help_text="Is the output written in the correct style or tone?",
    )

    helpfulness = models.IntegerField(
        choices=FivePointLikert.choices,
        null=True,
        default=None,
        blank=True,
        help_text="This output is helpful to the reader",
    )

    max_new_tokens = models.IntegerField(
        null=True,
        default=None,
        editable=True,
        blank=True,
        help_text="Set this field if different from the model configuration",
    )

    inference_on = models.DateTimeField(
        null=True,
        blank=True,
        default=None,
        help_text="The timestamp that the LLM started the inferencing",
    )

    inference_duration = models.DurationField(
        null=True,
        default=None,
        blank=True,
        help_text="The amount of time it took to perform the inference",
    )

    reviewers = models.ManyToManyField(
        "parley.ReviewTask", through="parley.ResponseReview"
    )

    class Meta:
        db_table = "responses"
        ordering = ("created",)
        get_latest_by = "created"
        verbose_name = "response"
        verbose_name_plural = "responses"
        unique_together = ("model", "prompt")

    def __str__(self):
        return f"{self.model.name} response for {str(self.prompt)}"

    @property
    def evaluation(self):
        return self.prompt.evaluation

    def agree_boolean(self, key, true_threshold=0.5):
        """
        Compute agreement for a boolean field across all user reviews. If threshold is
        between 0 and 1 then the proportion of reviews must have positive votes to be
        considered true. If the threshold is >= 1 then the fied must have that number
        of absolute votes to be considered true. None is returned if there are no
        reviews for this response to avoid misleading results.
        """

        if true_threshold < 0:
            raise ValueError("Threshold must be greater than or equal to 0")

        true_count = ResponseReview.objects.filter(response=self, **{key: True}).count()
        false_count = ResponseReview.objects.filter(
            response=self, **{key: False}
        ).count()
        if true_threshold < 1:
            # Consider the proportion of true votes
            total_count = true_count + false_count
            if total_count == 0:
                return None
            proportion = float(true_count) / float(total_count)
            return proportion >= true_threshold
        else:
            # Consider the absolute number of true votes
            if true_count >= true_threshold:
                return True
            if false_count >= true_threshold:
                return False
            return None

    def agree_likert(self, key, method="mean"):
        """
        Compute agreement for a likert scale field across all user reviews, either by "mean" or "median".
        """

        if method not in ("mean", "median"):
            raise ValueError("Method must be either mean or median")

        # Get all the reviews for this response
        reviews = (
            ResponseReview.objects.filter(response=self)
            .values_list(key, flat=True)
            .all()
        )

        # Filter out None values
        reviews = [review for review in reviews if review is not None]

        # If there are no reviews then return None
        if len(reviews) == 0:
            return None

        # Compute the mean or median of the reviews
        if method == "mean":
            return sum(reviews) / len(reviews)
        elif method == "median":
            return sorted(reviews)[len(reviews) // 2]

    def get_previous(self):
        try:
            qs = Response.objects.filter(
                model=self.model,
                prompt__evaluation=self.prompt.evaluation,
                prompt__order__lt=self.prompt.order,
            )
            return qs.order_by("-prompt__order").first()
        except self.DoesNotExist:
            return None

    def get_next(self):
        try:
            qs = Response.objects.filter(
                model=self.model,
                prompt__evaluation=self.prompt.evaluation,
                prompt__order__gt=self.prompt.order,
            )
            return qs.order_by("prompt__order").first()
        except self.DoesNotExist:
            return None

    def get_absolute_url(self):
        return reverse("response-detail", args=(self.id,))

    def get_pretty_output(self):
        # TODO: Handle other output format types
        if self.valid_output_type:
            if self.prompt.expected_output_type == OutputFormat.JSON:
                data = self.load_json()
                return json.dumps(data, indent=2)
        return self.output

    def load_json(self):
        output = self.output.strip()
        output = output.removeprefix("```json").removesuffix("```").strip()
        return json.loads(output)

    def check_valid_json(self):
        try:
            self.load_json()
            return True
        except json.JSONDecodeError:
            return False


class ModelEvaluation(BaseModel):
    """
    Models must be linked to specific evaluations in order to understand the
    performance of the model for the evaluation. Note that models are also linked to
    evaluations via their responses, but this model makes it easier to track aggregate
    data. This table represents a denormalization since the foreign keys are duplicated,
    but it makes application semantics a lot simpler.

    This table ensures there is a many to many relationship between models and
    evaluations.
    """

    model = models.ForeignKey(
        "parley.LLM",
        null=False,
        on_delete=models.CASCADE,
        related_name="model_evaluations",
        help_text="The LLM that needs to be evaluated",
    )

    evaluation = models.ForeignKey(
        "parley.Evaluation",
        null=False,
        on_delete=models.CASCADE,
        related_name="model_evaluations",
        help_text="The evaluation associated with the model",
    )

    reviewers = models.ManyToManyField(
        "auth.User",
        through="parley.ReviewTask",
    )

    # Cache Info
    metrics_cached = models.BooleanField(
        default=False,
        editable=False,
    )

    # Cache Info
    metrics_last_cached_on = models.DateTimeField(
        default=None,
        null=True,
        editable=False,
    )

    # Cached metric
    n_prompts = models.IntegerField(
        default=0,
        editable=False,
    )

    # Cached metric
    n_responses = models.IntegerField(
        default=0,
        editable=False,
    )

    # Cached metric
    n_similar = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_not_similar = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_labeled_correctly = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_labeled_incorrectly = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_valid_output_type = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_invalid_output_type = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_leaks_sensitive = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_no_sensitive_leaks = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_readable = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_not_readable = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_factual = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_not_factual = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_correct_style = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    n_incorrect_style = models.IntegerField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    mean_helpfulness = models.FloatField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    # Cached metric
    median_helpfulness = models.FloatField(
        default=None,
        null=True,
        blank=True,
        editable=False,
    )

    class Meta:
        db_table = "model_evaluations"
        ordering = ("-created",)
        get_latest_by = "created"
        unique_together = ("model", "evaluation")

    def prompts(self):
        return self.evaluation.prompts.filter(exclude=False)

    def responses(self):
        return Response.objects.filter(
            model=self.model,
            prompt__evaluation=self.evaluation,
            prompt__exclude=False,
        )

    @property
    def image(self):
        return self.model.image

    @property
    def percent_complete(self):
        return (
            float(self.responses().count()) / float(self.evaluation.prompts.count())
        ) * 100

    @property
    def similarity_processed(self):
        return self.n_similar is not None and self.n_not_similar is not None

    @property
    def similarity_normalized(self):
        return self._normalize(self.percent_similar)

    @property
    def percent_similar(self):
        return self._percent(self.n_similar, self.n_not_similar)

    @property
    def percent_not_similar(self):
        return self._percent(self.n_not_similar, self.n_similar)

    @property
    def labels_processed(self):
        return (
            self.n_labeled_correctly is not None
            and self.n_labeled_incorrectly is not None
        )

    @property
    def labels_normalized(self):
        return self._normalize(self.percent_labeled_correctly)

    @property
    def percent_labeled_correctly(self):
        return self._percent(self.n_labeled_correctly, self.n_labeled_incorrectly)

    @property
    def percent_labeled_incorrectly(self):
        return self._percent(self.n_labeled_incorrectly, self.n_labeled_correctly)

    @property
    def valid_output_processed(self):
        return (
            self.n_valid_output_type is not None
            and self.n_invalid_output_type is not None
        )

    @property
    def valid_output_normalized(self):
        return self._normalize(self.percent_valid_output_type)

    @property
    def percent_valid_output_type(self):
        return self._percent(self.n_valid_output_type, self.n_invalid_output_type)

    @property
    def percent_invalid_output_type(self):
        return self._percent(self.n_invalid_output_type, self.n_valid_output_type)

    @property
    def sensitive_processed(self):
        return (
            self.n_leaks_sensitive is not None and self.n_no_sensitive_leaks is not None
        )

    @property
    def sensitive_normalized(self):
        return self._normalize(self.percent_no_sensitive_leaks)

    @property
    def percent_leaks_sensitive(self):
        return self._percent(self.n_leaks_sensitive, self.n_no_sensitive_leaks)

    @property
    def percent_no_sensitive_leaks(self):
        return self._percent(self.n_no_sensitive_leaks, self.n_leaks_sensitive)

    @property
    def factual_processed(self):
        return self.n_factual is not None and self.n_not_factual is not None

    @property
    def factual_normalized(self):
        return self._normalize(self.percent_factual)

    @property
    def percent_factual(self):
        return self._percent(self.n_factual, self.n_not_factual)

    @property
    def percent_not_factual(self):
        return self._percent(self.n_not_factual, self.n_factual)

    @property
    def readability_processed(self):
        return self.n_readable is not None and self.n_not_readable is not None

    @property
    def readability_normalized(self):
        return self._normalize(self.percent_readable)

    @property
    def percent_readable(self):
        return self._percent(self.n_readable, self.n_not_readable)

    @property
    def percent_not_readable(self):
        return self._percent(self.n_not_readable, self.n_readable)

    @property
    def style_processed(self):
        return self.n_correct_style is not None and self.n_incorrect_style is not None

    @property
    def style_normalized(self):
        return self._normalize(self.percent_correct_style)

    @property
    def percent_correct_style(self):
        return self._percent(self.n_correct_style, self.n_incorrect_style)

    @property
    def percent_incorrect_style(self):
        return self._percent(self.n_incorrect_style, self.n_correct_style)

    @property
    def helpfulness_processed(self):
        return self.mean_helpfulness is not None and self.median_helpfulness is not None

    @property
    def mean_helpfulness_normalized(self):
        return self._normalize(self.mean_helpfulness, min_value=1, max_value=5)

    @property
    def median_helpfulness_normalized(self):
        return self._normalize(self.median_helpfulness, min_value=1, max_value=5)

    def __str__(self):
        return f"{self.evaluation.name} for {self.model.name}"

    def get_absolute_url(self):
        return reverse("llm-evaluation-detail", args=(self.id,))

    def _percent(self, field, counter_field):
        if field is None or counter_field is None:
            return None

        total = field + counter_field
        if total == 0:
            return 0.0

        return round((float(field) / float(total)) * 100, 2)

    def _normalize(self, value, min_value=0, max_value=100):
        """
        Normalize a value that exists in the range [min_value, max_value] to a value
        in the range [0, 1].
        """
        if value is None:
            return None
        if value < min_value or value > max_value:
            raise ValueError(
                f"Value {value} is outside the range [{min_value}, {max_value}]"
            )
        if min_value == max_value:
            return 0.0
        return round(
            (float(value) - float(min_value)) / (float(max_value) - float(min_value)), 2
        )
